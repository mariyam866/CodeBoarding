import logging

from langchain.prompts import PromptTemplate

from agents.agent import CodeBoardingAgent
from agents.agent_responses import AnalysisInsights, CFGAnalysisInsights, ValidationInsights
from agents.prompts import CFG_MESSAGE, SOURCE_MESSAGE, SYSTEM_MESSAGE, CONCLUSIVE_ANALYSIS_MESSAGE, FEEDBACK_MESSAGE


class AbstractionAgent(CodeBoardingAgent):
    def __init__(self, repo_dir, output_dir, cfg, project_name):
        super().__init__(repo_dir, output_dir, cfg, SYSTEM_MESSAGE)

        self.project_name = project_name

        self.context = {"structure_insight": []}  # Store evolving insights here

        self.prompts = {
            "cfg": PromptTemplate(template=CFG_MESSAGE, input_variables=["project_name", "cfg_str"]),
            "source": PromptTemplate(template=SOURCE_MESSAGE, input_variables=["insight_so_far"]),
            "final_analysis": PromptTemplate(template=CONCLUSIVE_ANALYSIS_MESSAGE,
                                             input_variables=["project_name", "cfg_insight", "structure_insight",
                                                              "source_insight"]),
            "feedback": PromptTemplate(template=FEEDBACK_MESSAGE, input_variables=["analysis", "feedback"])
        }

    def step_cfg(self, cfg_str):
        logging.info(f"[AbstractionAgent] Analyzing CFG for project: {self.project_name}")
        prompt = self.prompts["cfg"].format(project_name=self.project_name, cfg_str=cfg_str)
        parsed_response = self._parse_invoke(prompt, CFGAnalysisInsights)
        self.context['cfg_insight'] = parsed_response
        return parsed_response

    def step_source(self):
        logging.info(f"[AbstractionAgent] Analyzing Source for project: {self.project_name}")
        insight_str = ""
        for insight_type, analysis_insight in self.context.items():
            insight_str += f"## {insight_type.capitalize()} Insight\n"
            if type(analysis_insight) is list:
                insight_str += "\n".join([f"- {insight.llm_str()}" for insight in analysis_insight]) + "\n\n"
            else:
                insight_str += analysis_insight.llm_str() + "\n\n"

        prompt = self.prompts["source"].format(
            insight_so_far=insight_str,
        )
        parsed_response = self._parse_invoke(prompt, AnalysisInsights)
        self.context["source"] = parsed_response
        return parsed_response

    def generate_analysis(self):
        logging.info(f"[AbstractionAgent] Generating final analysis for project: {self.project_name}")
        prompt = self.prompts["final_analysis"].format(
            project_name=self.project_name,
            cfg_insight=self.context.get('cfg_insight').llm_str(),
            source_insight=self.context.get('source').llm_str()
        )
        analysis_result = self._parse_invoke(prompt, AnalysisInsights)
        return self.fix_source_code_reference_lines(analysis_result)

    def apply_feedback(self, analysis: AnalysisInsights, feedback: ValidationInsights):
        """
        Apply feedback to the analysis and return the updated analysis.
        This method should modify the analysis based on the feedback provided.
        """
        prompt = self.prompts["feedback"].format(analysis=analysis.llm_str(), feedback=feedback)
        analysis = self._parse_invoke(prompt, AnalysisInsights)
        return self.fix_source_code_reference_lines(analysis)

    def run(self, cfg_str):
        self.step_cfg(cfg_str)
        self.step_source()
        return self.generate_analysis()
